{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JoQT5dRD2QYHixvwLLukNn0Dgzc2rlmC","timestamp":1669649103318},{"file_id":"18ME26bXW5jfsJbI14iwgoMcDDutr_Yp2","timestamp":1636648306777},{"file_id":"1JTGUWoDJzMU8XNR0AlhRJbPCCNvDB6lT","timestamp":1636044462410},{"file_id":"1UIsJYaUP28Y_TIIF2LihAkBKJ1ZDdBkm","timestamp":1636041668950},{"file_id":"1wLo-su7vjcbreZcnvGxFRAHL1O9YcYSn","timestamp":1635443578663},{"file_id":"1Bd2DQyzYjUKQ5VeieeR18-YT-NbjTStV","timestamp":1635442112671},{"file_id":"1iLlKPepef6CnasWLnYPAMSLTDA1LiWbx","timestamp":1634830959365}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"qyWjqitA2ie8","executionInfo":{"status":"ok","timestamp":1669652527100,"user_tz":-60,"elapsed":60949,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7895960c-70b9-4bef-a32c-af68787de395"},"source":["appname = \"Dog_food_assignment\"\n","\n","# Look into https://spark.apache.org/downloads.html for the latest version\n","spark_mirror = \"https://mirrors.sonic.net/apache/spark\"\n","spark_version = \"3.3.1\"\n","hadoop_version = \"3\"\n","\n","# Install Java 8 (Spark does not work with newer Java versions)\n","! apt-get update\n","! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Download and extract Spark binary distribution\n","! rm -rf spark-{spark_version}-bin-hadoop{hadoop_version}.tgz spark-{spark_version}-bin-hadoop{hadoop_version}\n","! wget -q {spark_mirror}/spark-{spark_version}/spark-{spark_version}-bin-hadoop{hadoop_version}.tgz\n","! tar xzf spark-{spark_version}-bin-hadoop{hadoop_version}.tgz\n","\n","# The only 2 environment variables needed to set up Java and Spark\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/spark-{spark_version}-bin-hadoop{hadoop_version}\"\n","\n","# Set up the Spark environment based on the environment variable SPARK_HOME\n","! pip install -q findspark\n","import findspark\n","findspark.init()\n","\n","# Get the Spark session object (basic entry point for every operation)\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(appname).master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n","Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [1,038 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n","Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,497 kB]\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,563 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,338 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,303 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,069 kB]\n","Fetched 13.1 MB in 6s (2,181 kB/s)\n","Reading package lists... Done\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j2jrXRnC9O_","executionInfo":{"status":"ok","timestamp":1669652564884,"user_tz":-60,"elapsed":26128,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"0be79275-4dd3-45b9-d079-7cf900d17f8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"FdCdt1XO3D3b"},"source":["df = spark.read.format('csv').options(inferSchema=True, header=True).load('/content/drive/MyDrive/Colab Notebooks/dog_food.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Printing some basic stats about our data"],"metadata":{"id":"AeezF0-KDgQb"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yr6krMiYXBcc","executionInfo":{"status":"ok","timestamp":1669652669123,"user_tz":-60,"elapsed":2369,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"1f4921f8-b0fc-4ea2-e71f-09d67378030a"},"source":["df.describe().show()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------------+------------------+------------------+------------------+-------------------+\n","|summary|                 A|                 B|                 C|                 D|            Spoiled|\n","+-------+------------------+------------------+------------------+------------------+-------------------+\n","|  count|               490|               490|               490|               490|                490|\n","|   mean|  5.53469387755102| 5.504081632653061| 9.126530612244897| 5.579591836734694| 0.2857142857142857|\n","| stddev|2.9515204234399057|2.8537966089662063|2.0555451971054275|2.8548369309982857|0.45221563164613465|\n","|    min|                 1|                 1|               5.0|                 1|                0.0|\n","|    max|                10|                10|              14.0|                10|                1.0|\n","+-------+------------------+------------------+------------------+------------------+-------------------+\n","\n"]}]},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvqVelgcECvC","executionInfo":{"status":"ok","timestamp":1669652859994,"user_tz":-60,"elapsed":13,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"4570becf-4bcd-4642-880d-8611e30be161"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- A: integer (nullable = true)\n"," |-- B: integer (nullable = true)\n"," |-- C: double (nullable = true)\n"," |-- D: integer (nullable = true)\n"," |-- Spoiled: double (nullable = true)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"vEKsJsAZ3Qo3"},"source":["from pyspark.ml.feature import VectorAssembler\n","assembler = VectorAssembler(inputCols=['A','B','C','D'], outputCol='features')\n","\n","final_data = assembler.transform(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_data.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aLT0SlkoAfCZ","executionInfo":{"status":"ok","timestamp":1669653765728,"user_tz":-60,"elapsed":7,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"eac526bc-9a8c-4c86-b884-d2843cd16dcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+----+---+-------+------------------+\n","|  A|  B|   C|  D|Spoiled|          features|\n","+---+---+----+---+-------+------------------+\n","|  4|  2|12.0|  3|    1.0|[4.0,2.0,12.0,3.0]|\n","|  5|  6|12.0|  7|    1.0|[5.0,6.0,12.0,7.0]|\n","|  6|  2|13.0|  6|    1.0|[6.0,2.0,13.0,6.0]|\n","|  4|  2|12.0|  1|    1.0|[4.0,2.0,12.0,1.0]|\n","|  4|  2|12.0|  3|    1.0|[4.0,2.0,12.0,3.0]|\n","+---+---+----+---+-------+------------------+\n","only showing top 5 rows\n","\n"]}]},{"cell_type":"code","metadata":{"id":"XDEJB5mP2jkD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669659654755,"user_tz":-60,"elapsed":10,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"f0d6ca89-5410-4a78-c93e-43e4980e17c6"},"source":["from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier,\n","                                       DecisionTreeClassifier)\n","print(RandomForestClassifier().explainParams())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n","cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n","checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n","featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n","featuresCol: features column name. (default: features)\n","impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n","labelCol: label column name. (default: label)\n","leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n","maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n","maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n","maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n","minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n","minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n","minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n","numTrees: Number of trees to train (>= 1). (default: 20)\n","predictionCol: prediction column name. (default: prediction)\n","probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n","rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n","seed: random seed. (default: -1216434367422647036)\n","subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n","thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n","weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"]}]},{"cell_type":"code","metadata":{"id":"7IxdcQHM3XSI"},"source":["train, test = final_data.randomSplit([0.75, 0.25])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrePsp283Yqk"},"source":["dtc = DecisionTreeClassifier(labelCol = \"Spoiled\", impurity=\"entropy\",seed=-476609685677241813)\n","rfc = RandomForestClassifier(labelCol = \"Spoiled\", numTrees = 100, impurity=\"entropy\", seed=-476609685677241813)\n","gbt = GBTClassifier(labelCol = \"Spoiled\", featuresCol = \"features\", seed=-476609685677241813)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8kjF0QJEVd_"},"source":["dtc_model = dtc.fit(train)\n","rfc_model = rfc.fit(train)\n","gbt_model = gbt.fit(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Syd8X5lcE5r6"},"source":["dtc_preds = dtc_model.transform(test)\n","rfc_preds = rfc_model.transform(test)\n","gbt_preds = gbt_model.transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDi4OnMxvXxu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669654611035,"user_tz":-60,"elapsed":400,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"cbc33234-ad4c-4e3c-e95a-8232c00ffa17"},"source":["dtc_preds.show(10)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---+----+---+-------+-------------------+-------------+--------------------+----------+\n","|  A|  B|   C|  D|Spoiled|           features|rawPrediction|         probability|prediction|\n","+---+---+----+---+-------+-------------------+-------------+--------------------+----------+\n","|  1|  1|12.0|  4|    1.0| [1.0,1.0,12.0,4.0]|   [0.0,86.0]|           [0.0,1.0]|       1.0|\n","|  1|  2| 9.0|  1|    0.0|  [1.0,2.0,9.0,1.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","|  1|  4| 9.0|  6|    0.0|  [1.0,4.0,9.0,6.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","|  1|  5|12.0| 10|    1.0|[1.0,5.0,12.0,10.0]|   [0.0,10.0]|           [0.0,1.0]|       1.0|\n","|  1|  8| 7.0| 10|    0.0| [1.0,8.0,7.0,10.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","|  1|  8| 8.0|  8|    0.0|  [1.0,8.0,8.0,8.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","|  1|  9| 7.0|  5|    0.0|  [1.0,9.0,7.0,5.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","|  1|  9|10.0|  6|    0.0| [1.0,9.0,10.0,6.0]|    [0.0,1.0]|           [0.0,1.0]|       1.0|\n","|  2|  1| 8.0|  9|    0.0|  [2.0,1.0,8.0,9.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","|  2|  1| 9.0|  1|    0.0|  [2.0,1.0,9.0,1.0]|  [240.0,1.0]|[0.99585062240663...|       0.0|\n","+---+---+----+---+-------+-------------------+-------------+--------------------+----------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"markdown","source":["Testing the accuracy of our model"],"metadata":{"id":"o6rnaPZnGYTw"}},{"cell_type":"code","metadata":{"id":"tFggKy04GtD6"},"source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","evaluator = MulticlassClassificationEvaluator(metricName='accuracy', labelCol = \"Spoiled\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tF_-TDmmG8Ls","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669654616693,"user_tz":-60,"elapsed":743,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"5b4e479e-987f-4b3d-f30e-1eee9ba605a2"},"source":["print(f'DTC: {evaluator.evaluate(dtc_preds)}')\n","print(f'RFC: {evaluator.evaluate(rfc_preds)}')\n","print(f'GBT: {evaluator.evaluate(gbt_preds)}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DTC: 0.9596774193548387\n","RFC: 0.9758064516129032\n","GBT: 0.967741935483871\n"]}]},{"cell_type":"markdown","source":["These accuracies are very high, which helps us consider they have a closeness to reality. Therefore, we can extract what variables was the most influential in the predictions"],"metadata":{"id":"HFzE2QIzGbuJ"}},{"cell_type":"code","source":["#Found in https://stackoverflow.com/questions/28971989/pyspark-mllib-random-forest-feature-importances\n","va = assembler\n","#display(dtc_model)\n","#print(dtc_model.toDebugString) #print the nodes of the decision tree model\n","\n","list(zip(va.getInputCols(), dtc_model.featureImportances, rfc_model.featureImportances, gbt_model.featureImportances),)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ayeLseRGW6D","executionInfo":{"status":"ok","timestamp":1669655566451,"user_tz":-60,"elapsed":422,"user":{"displayName":"Pablo M.","userId":"08716658997747174273"}},"outputId":"cc0a2695-43f5-462f-fd8e-af9ed35a426d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('A', 0.014005953378647792, 0.024730775253754205, 0.03711618878251018),\n"," ('B', 0.0025391503587106233, 0.020541113442031023, 0.052742905096166956),\n"," ('C', 0.9825814285392451, 0.9271062101149683, 0.8764795573465588),\n"," ('D', 0.0008734677233964758, 0.027621901189246473, 0.033661348774764024)]"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","source":["As it can be seen, the most influential chemical was C with an importance of:\n","\n","*   DecisionTreeClassifier: 98%\n","*   RandomTreeClassifier: 93%\n","*   GBTClassifier: 88%\n","\n","Therefore, chemical C should be the one affecting food spoiling.\n","\n","\n"],"metadata":{"id":"GK_Ko5VHN-Aa"}}]}